{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b1f2a19c",
   "metadata": {},
   "source": [
    "# one-hot编码的实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6a5c15e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#导入用于对象保存的与加载的joblib\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2951c897",
   "metadata": {},
   "outputs": [],
   "source": [
    "#导入keras中的词汇映射器Tokenizer\n",
    "from keras.preprocessing.text import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c6134351",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = {\"周杰伦\", \"陈奕迅\", \"王力宏\", \"李宗盛\", \"吴亦凡\", \"鹿晗\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "40b16d95",
   "metadata": {},
   "outputs": [],
   "source": [
    "#实例化词汇映射器，num_word=None默认处理所有单词\n",
    "t = Tokenizer(num_words=None, char_level=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "95a250e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#使用词汇映射器拟合现有的文本数据\n",
    "t.fit_on_texts(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9e3970b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "吴亦凡 的one-hot编码为：  [1, 0, 0, 0, 0, 0]\n",
      "李宗盛 的one-hot编码为：  [0, 1, 0, 0, 0, 0]\n",
      "周杰伦 的one-hot编码为：  [0, 0, 1, 0, 0, 0]\n",
      "王力宏 的one-hot编码为：  [0, 0, 0, 1, 0, 0]\n",
      "陈奕迅 的one-hot编码为：  [0, 0, 0, 0, 1, 0]\n",
      "鹿晗 的one-hot编码为：  [0, 0, 0, 0, 0, 1]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['./Tokenizer']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#循环遍历词汇表，映射每个单词\n",
    "for token in vocab:\n",
    "    #设置词向量的长度\n",
    "    zero_list = [0]*len(vocab)\n",
    "    #使用映射器转化文本数据，每个词汇对应从1开始\n",
    "    token_indx = t.texts_to_sequences([token])[0][0]-1\n",
    "    #将词向量的对应位置的索引值设置为1\n",
    "    zero_list[token_indx] = 1\n",
    "    print(token, '的one-hot编码为： ', zero_list)\n",
    "\n",
    "tokenizer_path = './Tokenizer'\n",
    "joblib.dump(t, tokenizer_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6493abcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "李宗盛 的one-hot是： [0, 1, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "t = joblib.load('./Tokenizer')\n",
    "token = '李宗盛'\n",
    "token_index = t.texts_to_sequences([token])[0][0] -1\n",
    "zero_list = [0]*6\n",
    "zero_list[token_index] = 1\n",
    "print(token,'的one-hot是：',zero_list)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "533b8c7f",
   "metadata": {},
   "source": [
    "one-shot总结：\n",
    "    one-hot编码的优劣势：\n",
    "    优势：操作简单，容易理解.\n",
    "    劣势：完全割裂了词与词之间的联系，而且在大语料集下，每个向量的长度过大，占据大量内存.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebc1d1ba",
   "metadata": {},
   "source": [
    "# 什么是word2vec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79102064",
   "metadata": {},
   "source": [
    "是一种流行的将词汇表示成向量的无监督训练方法, 该过程将构建神经网络模型,\n",
    "将网络参数作为词汇的向量表示, 它包含CBOW和skipgram两种训练模式."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "53fd8de2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d2462f70",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "751e3b2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class word2vec():\n",
    "    def __init__(self):\n",
    "        self.n = setting['n']\n",
    "        self.lr = setting['learning_rate']\n",
    "        self.epochs = setting['epochs']\n",
    "        self.window = setting['window_size']\n",
    "        \n",
    "    def generate_training_data(self, setting, corpus):\n",
    "        '''得到训练数据'''\n",
    "        \n",
    "        #一个字典，当访问的键不存在时，用int类型实例化一个默认值\n",
    "        word_counts = defaultdict(int)\n",
    "        \n",
    "        #遍历语料库corpus\n",
    "        for row in corpus:\n",
    "            for word in row:\n",
    "                #统计每个单词的出现次数\n",
    "                word_counts[word] += 1\n",
    "        \n",
    "        #词汇表的长度       \n",
    "        self.v_count = len(word_counts.keys())\n",
    "        #在词汇表中的单词组成的列表\n",
    "        self.words_list = list(word_counts.keys())\n",
    "        #以词汇表中单词为key，索引为value的字典数据\n",
    "        self.word_index = dict((word, i) for i, word in enumerate(self.words_list))\n",
    "        #以索引为key，以词汇表中的单词为字典数据\n",
    "        self.index_word = dict((i, word) for i, word in enumerate(self.words_list))\n",
    "        \n",
    "        train_data = []\n",
    "        \n",
    "        for sentence in corpus:\n",
    "            sent_len = len(sentence)\n",
    "            \n",
    "            for i, word in enumerate(sentence):\n",
    "            \n",
    "                w_target = self.word2onehot(sentence[i])\n",
    "            \n",
    "                w_context = []\n",
    "            \n",
    "            for j in range(i - self.window, i+self.window):\n",
    "                if j != i and j <= sent_len - 1 and j >= 0:\n",
    "                    w_context.append(self.word2onehot(sentence[j]))\n",
    "            \n",
    "            train_data.append([w_target, w_context])\n",
    "        \n",
    "        return np.array(train_data)\n",
    "    \n",
    "    def word2onehot(self, word):\n",
    "        #将单词用one-hot编码\n",
    "        \n",
    "        word_vec = [0 for i in range(0, self.v_count)]\n",
    "        #获取单词的在字典中的索引值\n",
    "        word_index = self.word_index[word]\n",
    "        #将onehot编码中该位置的值设置为1\n",
    "        word_vec[word_index] = 1\n",
    "        return word_vec\n",
    "    \n",
    "    def train(self, train_data):\n",
    "        \n",
    "        #随机参数化w1,w2\n",
    "        self.w1 = np.random.uniform(-1, 1, (self.v_count, self.n))\n",
    "        \n",
    "        self.w2 = np.random.uniform(-1, 1, (self.n, self.v_count))\n",
    "        \n",
    "        for i in range(self.epochs):\n",
    "            self.loss = 0\n",
    "            # w_t 是表示目标词的one-hot向量\n",
    "            #w_t -> w_target,w_c ->w_context\n",
    "            for w_t, w_c in train_data:\n",
    "                #前向传播\n",
    "                y_pred, h, u = self.forward(w_t)\n",
    "                \n",
    "                #计算误差\n",
    "                EI = np.sum([np.subtract(y_pred, word) for word in w_c], axis=0)\n",
    "                \n",
    "                #反向传播，更新参数\n",
    "                self.backprop(EI, h, w_t)\n",
    "\n",
    "                #计算总损失\n",
    "                self.loss += -np.sum([u[word.index(1)] for word in w_c]) + len(w_c) * np.log(np.sum(np.exp(u)))\n",
    "                \n",
    "            print('Epoch: ',i, 'loss:', self.loss)\n",
    "            \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        前向传播\n",
    "        \"\"\"\n",
    " \n",
    "        h = np.dot(self.w1.T, x)\n",
    " \n",
    "        u = np.dot(self.w2.T, h)\n",
    " \n",
    "        y_c = self.softmax(u)\n",
    " \n",
    "        return y_c, h, u\n",
    "    \n",
    "    def softmax(self, x):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        e_x = np.exp(x - np.max(x))\n",
    " \n",
    "        return e_x / np.sum(e_x)\n",
    "\n",
    "    def backprop(self, e, h, x):\n",
    " \n",
    "        d1_dw2 = np.outer(h, e)\n",
    "        d1_dw1 = np.outer(x, np.dot(self.w2, e.T))\n",
    " \n",
    "        self.w1 = self.w1 - (self.lr * d1_dw1)\n",
    "        self.w2 = self.w2 - (self.lr * d1_dw2)\n",
    "    \n",
    "    def word_vec(self, word):\n",
    " \n",
    "        \"\"\"\n",
    "        获取词向量\n",
    "        通过获取词的索引直接在权重向量中找\n",
    "        \"\"\"\n",
    " \n",
    "        w_index = self.word_index[word]\n",
    "        v_w = self.w1[w_index]\n",
    " \n",
    "        return v_w\n",
    "\n",
    "    def vec_sim(self, word, top_n):\n",
    "        \"\"\"\n",
    "        找相似的词\n",
    "        \"\"\"\n",
    " \n",
    "        v_w1 = self.word_vec(word)\n",
    "        word_sim = {}\n",
    " \n",
    "        for i in range(self.v_count):\n",
    "            v_w2 = self.w1[i]\n",
    "            theta_sum = np.dot(v_w1, v_w2)\n",
    " \n",
    "            #np.linalg.norm(v_w1) 求范数 默认为2范数，即平方和的二次开方\n",
    "            theta_den = np.linalg.norm(v_w1) * np.linalg.norm(v_w2)\n",
    "            theta = theta_sum / theta_den\n",
    " \n",
    "            word = self.index_word[i]\n",
    "            word_sim[word] = theta\n",
    " \n",
    "        words_sorted = sorted(word_sim.items(), key=lambda kv: kv[1], reverse=True)\n",
    " \n",
    "        for word, sim in words_sorted[:top_n]:\n",
    "            print(word, sim)\n",
    " \n",
    "    def get_w(self):\n",
    "        w1 = self.w1\n",
    "        return  w1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "5a2166a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#数据准备\n",
    "text = \"natural language processing and machine learning is fun and exciting\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "29e6e83e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jieba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "53c87501",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = jieba.lcut(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "0e323e63",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus.remove(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "c6b69d51",
   "metadata": {},
   "outputs": [],
   "source": [
    "for corpu in corpus:\n",
    "    if corpu == ' ':\n",
    "        corpus.remove(corpu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "4f4d64dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['natural',\n",
       " 'language',\n",
       " 'processing',\n",
       " 'and',\n",
       " 'machine',\n",
       " 'learning',\n",
       " 'is',\n",
       " 'fun',\n",
       " 'and',\n",
       " 'exciting']"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "9eebd336",
   "metadata": {},
   "outputs": [],
   "source": [
    "#超参数\n",
    "setting = {\n",
    "    'window_size': 2,   #窗口尺寸 m\n",
    "    #单词嵌入(word embedding)的维度,维度也是隐藏层的大小。\n",
    "    'n': 10,\n",
    "    'epochs': 50,         #表示遍历整个样本的次数。在每个epoch中，我们循环通过一遍训练集的样本。\n",
    "    'learning_rate':0.01 #学习率\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "c7dfceaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#初始化一个Word2vec对象\n",
    "w2v = word2vec()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "2f820028",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_14120\\1380975523.py:46: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  return np.array(train_data)\n"
     ]
    }
   ],
   "source": [
    "train_data = w2v.generate_training_data(setting, corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "49943f12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  0 loss: 65.28186179067046\n",
      "Epoch:  1 loss: 62.49333835626412\n",
      "Epoch:  2 loss: 59.95709553684452\n",
      "Epoch:  3 loss: 57.63565412785316\n",
      "Epoch:  4 loss: 55.498082361473365\n",
      "Epoch:  5 loss: 53.518648662126864\n",
      "Epoch:  6 loss: 51.67580903485525\n",
      "Epoch:  7 loss: 49.95146176276482\n",
      "Epoch:  8 loss: 48.33041115943818\n",
      "Epoch:  9 loss: 46.79998751999064\n",
      "Epoch:  10 loss: 45.34977429919802\n",
      "Epoch:  11 loss: 43.97139844728465\n",
      "Epoch:  12 loss: 42.658347017461686\n",
      "Epoch:  13 loss: 41.40578233532541\n",
      "Epoch:  14 loss: 40.210337981378395\n",
      "Epoch:  15 loss: 39.06988731181349\n",
      "Epoch:  16 loss: 37.98328443798145\n",
      "Epoch:  17 loss: 36.95008428468836\n",
      "Epoch:  18 loss: 35.97025367571422\n",
      "Epoch:  19 loss: 35.04388945515912\n",
      "Epoch:  20 loss: 34.170962218311935\n",
      "Epoch:  21 loss: 33.35110464793235\n",
      "Epoch:  22 loss: 32.58346091791102\n",
      "Epoch:  23 loss: 31.866607721561916\n",
      "Epoch:  24 loss: 31.198548774853705\n",
      "Epoch:  25 loss: 30.576774866701072\n",
      "Epoch:  26 loss: 29.998373095746654\n",
      "Epoch:  27 loss: 29.46016399736382\n",
      "Epoch:  28 loss: 28.958844824342655\n",
      "Epoch:  29 loss: 28.491120828282988\n",
      "Epoch:  30 loss: 28.053812426959865\n",
      "Epoch:  31 loss: 27.643932730689755\n",
      "Epoch:  32 loss: 27.258735501010904\n",
      "Epoch:  33 loss: 26.895737415127037\n",
      "Epoch:  34 loss: 26.5527204034218\n",
      "Epoch:  35 loss: 26.227720174976753\n",
      "Epoch:  36 loss: 25.919006391155648\n",
      "Epoch:  37 loss: 25.625058807123885\n",
      "Epoch:  38 loss: 25.344542460503785\n",
      "Epoch:  39 loss: 25.07628387626368\n",
      "Epoch:  40 loss: 24.81924938051204\n",
      "Epoch:  41 loss: 24.572525990068925\n",
      "Epoch:  42 loss: 24.335304939253803\n",
      "Epoch:  43 loss: 24.10686767129491\n",
      "Epoch:  44 loss: 23.886574008881887\n",
      "Epoch:  45 loss: 23.673852183755695\n",
      "Epoch:  46 loss: 23.468190416215172\n",
      "Epoch:  47 loss: 23.2691297693073\n",
      "Epoch:  48 loss: 23.076258044735017\n",
      "Epoch:  49 loss: 22.88920452972211\n"
     ]
    }
   ],
   "source": [
    "#训练\n",
    "w2v.train(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "cd286d30",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'machine'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Input \u001b[1;32mIn [79]\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m#获取词的向量\u001b[39;00m\n\u001b[0;32m      2\u001b[0m word \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmachine\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m----> 3\u001b[0m vec \u001b[38;5;241m=\u001b[39m \u001b[43mw2v\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mword_vec\u001b[49m\u001b[43m(\u001b[49m\u001b[43mword\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(word, vec)\n",
      "Input \u001b[1;32mIn [58]\u001b[0m, in \u001b[0;36mword2vec.word_vec\u001b[1;34m(self, word)\u001b[0m\n\u001b[0;32m    112\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mword_vec\u001b[39m(\u001b[38;5;28mself\u001b[39m, word):\n\u001b[0;32m    114\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    115\u001b[0m \u001b[38;5;124;03m    获取词向量\u001b[39;00m\n\u001b[0;32m    116\u001b[0m \u001b[38;5;124;03m    通过获取词的索引直接在权重向量中找\u001b[39;00m\n\u001b[0;32m    117\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 119\u001b[0m     w_index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mword_index\u001b[49m\u001b[43m[\u001b[49m\u001b[43mword\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m    120\u001b[0m     v_w \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mw1[w_index]\n\u001b[0;32m    122\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m v_w\n",
      "\u001b[1;31mKeyError\u001b[0m: 'machine'"
     ]
    }
   ],
   "source": [
    "#获取词的向量\n",
    "word = 'machine'\n",
    "vec = w2v.word_vec(word)\n",
    "print(word, vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31303e9d",
   "metadata": {},
   "source": [
    "## word2vce的简单实现-SkipGram模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "2b3a6233",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.utils.data as data\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "a7b3d4c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.FloatTensor"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dtype = torch.FloatTensor\n",
    "dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afe24e69",
   "metadata": {},
   "source": [
    "## 文本预处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "7967a018",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [\"jack like dog\", \"jack like cat\", \"jack like animal\",\n",
    "  \"dog cat animal\", \"banana apple cat dog like\", \"dog fish milk like\",\n",
    "  \"dog cat animal like\", \"jack like apple\", \"apple like\", \"jack like banana\",\n",
    "  \"apple banana jack movie book music like\", \"cat dog hate\", \"cat dog like\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "59cc81dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#''.join(sentence)将句子中的所有字符以空格连接起来\n",
    "word_sequence = ' '.join(sentences).split()\n",
    "#去除重复单词并放入列表\n",
    "vocab = list(set(word_sequence))\n",
    "#转为索引与单词对应的字典\n",
    "word2idx = {w: i for i, w in enumerate(vocab)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "a53d9608",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'banana': 0,\n",
       " 'like': 1,\n",
       " 'dog': 2,\n",
       " 'apple': 3,\n",
       " 'milk': 4,\n",
       " 'fish': 5,\n",
       " 'music': 6,\n",
       " 'animal': 7,\n",
       " 'hate': 8,\n",
       " 'jack': 9,\n",
       " 'movie': 10,\n",
       " 'book': 11,\n",
       " 'cat': 12}"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2idx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66f6bbf9",
   "metadata": {},
   "source": [
    "## 模型相关参数设置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "660143fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 8\n",
    "#2 dim vector represent one word\n",
    "embedding_size = 2 \n",
    "#window size\n",
    "C = 2\n",
    "#词表长度\n",
    "voc_size = len(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cac78a0",
   "metadata": {},
   "source": [
    "## 数据预处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "d3d1071a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#1\n",
    "skip_grams = []\n",
    "for idx in range(C, len(word_sequence) - C):\n",
    "    #center word 获取中心词的数值表示\n",
    "    center = word2idx[word_sequence[idx]]\n",
    "    #中心词的左右范围内词\n",
    "    context_idx = list(range(idx - C, idx)) + list(range(idx + 1, idx + C + 1))\n",
    "    #左右四个词的数值表示\n",
    "    context = [word2idx[word_sequence[i]] for i in context_idx]\n",
    "    #将中心词的数值与左右四个词的数值分别组对\n",
    "    for w in context:\n",
    "        skip_grams.append([center, w])\n",
    "#2\n",
    "def make_data(skip_grams):\n",
    "    input_data = []\n",
    "    output_data = []\n",
    "    for i in range(len(skip_grams)):\n",
    "        #获取中心词对应的独热编码\n",
    "        input_data.append(np.eye(voc_size)[skip_grams[i][0]])\n",
    "        #中心对应的上下文词的数值，作为与中心词配对的标签\n",
    "        output_data.append(skip_grams[i][1])\n",
    "    return input_data, output_data\n",
    "#3\n",
    "input_data, output_data =make_data(skip_grams)\n",
    "input_data, output_data = torch.Tensor(input_data), torch.LongTensor(output_data)\n",
    "#将输入与输出装入datatset\n",
    "dataset = data.TensorDataset(input_data, output_data)\n",
    "loader = data.DataLoader(dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbf03ea4",
   "metadata": {},
   "source": [
    "## 构建模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "3a807f2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Word2Vec(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Word2Vec, self).__init__()\n",
    "        \n",
    "        \n",
    "#         torch.nn.Parameter是继承自torch.Tensor的子类，其主要作用是作为nn.Module中的可训练参数使用。\n",
    "#         它与torch.Tensor的区别就是nn.Parameter会自动被认为是module的可训练参数，\n",
    "#         即加入到parameter()这个迭代器中去；\n",
    "#         而module中非nn.Parameter()的普通tensor是不在parameter中的。\n",
    "#         nn.Parameter的对象的requires_grad属性的默认值是True，即是可被训练的，\n",
    "#        这与torth.Tensor对象的默认值相反\n",
    "        \n",
    "        #与self-attention中的自乘矩阵类似，是可变的，会根据梯度下降不断调整\n",
    "        #中间矩阵W，V\n",
    "        self.W = nn.Parameter(torch.randn(voc_size, embedding_size).type(dtype))\n",
    "        self.V = nn.Parameter(torch.randn(embedding_size, voc_size).type(dtype))\n",
    "        \n",
    "    def forward(self, X):\n",
    "        #X:[batch_size, voc_size] ont-hot\n",
    "        # torch.mm only for 2 dim matrix, but torch.matmul can use to any dim\n",
    "        \n",
    "        hidden_layer = torch.matmul(X, self.W) # hidden_layer : [batch_size, embedding_size]\n",
    "        output_layer = torch.matmul(hidden_layer, self.V) # output_layer : [batch_size, voc_size]\n",
    "        return output_layer\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "d0a0ad74",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available else 'cpu'\n",
    "model = Word2Vec()\n",
    "model.to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61c66774",
   "metadata": {},
   "source": [
    "## 训练模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "9784c1e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000 0 1.8375694751739502\n",
      "1000 1 1.738577961921692\n",
      "1000 2 1.8467507362365723\n",
      "1000 3 2.8312172889709473\n",
      "1000 4 2.6680169105529785\n",
      "1000 5 1.8381412029266357\n",
      "1000 6 1.7617523670196533\n",
      "1000 7 1.8250060081481934\n",
      "1000 8 2.051676034927368\n",
      "1000 9 2.2264623641967773\n",
      "1000 10 1.8403165340423584\n",
      "1000 11 1.8901629447937012\n",
      "1000 12 2.1053130626678467\n",
      "1000 13 2.0844287872314453\n",
      "1000 14 2.266641139984131\n",
      "1000 15 1.8731367588043213\n",
      "1000 16 2.3490219116210938\n",
      "1000 17 2.173323392868042\n",
      "1000 18 2.5230770111083984\n",
      "1000 19 1.9397269487380981\n",
      "1000 20 2.0467922687530518\n",
      "2000 0 1.7716821432113647\n",
      "2000 1 2.0466668605804443\n",
      "2000 2 2.414527654647827\n",
      "2000 3 1.980623483657837\n",
      "2000 4 1.8269540071487427\n",
      "2000 5 1.860558271408081\n",
      "2000 6 2.006382703781128\n",
      "2000 7 1.96543550491333\n",
      "2000 8 1.9728012084960938\n",
      "2000 9 1.9555045366287231\n",
      "2000 10 2.1772289276123047\n",
      "2000 11 2.2595701217651367\n",
      "2000 12 2.3065390586853027\n",
      "2000 13 2.077195405960083\n",
      "2000 14 2.2358155250549316\n",
      "2000 15 1.829107403755188\n",
      "2000 16 2.205472469329834\n",
      "2000 17 2.229891300201416\n",
      "2000 18 1.9017822742462158\n",
      "2000 19 2.477797031402588\n",
      "2000 20 1.76825749874115\n"
     ]
    }
   ],
   "source": [
    "epochs = 2000\n",
    "for epoch in range(epochs):\n",
    "    for i, (batch_x, batch_y) in enumerate(loader):\n",
    "        batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
    "        pred = model(batch_x)\n",
    "        loss = criterion(pred, batch_y)\n",
    "        \n",
    "        if (epoch + 1) % 1000 == 0:\n",
    "            print(epoch + 1, i, loss.item())\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd5d6c28",
   "metadata": {},
   "source": [
    "由于我这里每个词是用的 2 维的向量去表示，所以可以将每个词在平面直角坐标系中标记出来，看看各个词之间的距离"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "5f2b374e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "for i, label in enumerate(vocab):\n",
    "    W, WT = model.parameters()\n",
    "    x, y = float(W[i][0]), float(W[i][1])\n",
    "    plt.scatter(x, y)\n",
    "    plt.annotate(label, xy=(x, y), xytext = (5, 2), textcoords='offset points', ha='right', va='bottom')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "425c7785",
   "metadata": {},
   "source": [
    "# word2vce的简单实现-CBOW模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "290ce9d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "03947854",
   "metadata": {},
   "outputs": [],
   "source": [
    "dtype = torch.FloatTensor\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "8498a894",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [\"jack like dog\", \"jack like cat\", \"jack like animal\",\n",
    "  \"dog cat animal\", \"banana apple cat dog like\", \"dog fish milk like\",\n",
    "  \"dog cat animal like\", \"jack like apple\", \"apple like\", \"jack like banana\",\n",
    "  \"apple banana jack movie book music like\", \"cat dog hate\", \"cat dog like\"]\n",
    "\n",
    "word_sequence = \" \".join(sentences).split() # ['jack', 'like', 'dog', 'jack', 'like', 'cat', 'animal',...]\n",
    "vocab = list(set(word_sequence)) # build words vocabulary\n",
    "word2idx = {w: i for i, w in enumerate(vocab)} # {'jack':0, 'like':1,...}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "5bcab3a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word2Vec Parameters\n",
    "batch_size = 8\n",
    "embedding_size = 2  # 2 dim vector represent one word\n",
    "C = 2 # window size\n",
    "voc_size = len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06400b1a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "321b8603",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4506e7bc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28bb6d03",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f037f59",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a292277",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
